{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9387e9e37a0af8fff0fcbade4737076ebc87d0c8fec44545ee3d2fd05cdc73df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import models,layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'his', 'are', 'have', 'be', 'he', 'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'more', 'when', 'very', 'she', 'even', 'my', 'no', 'would', 'up', 'time', 'only', 'which', 'story', 'really', 'their', 'were', 'had', 'see', 'can', 'me', 'than', 'we', 'much', 'well', 'get', 'been', 'will', 'into', 'people', 'also', 'other', 'do', 'bad', 'because', 'great', 'first', 'how', 'him', 'most', 'dont', 'made', 'then', 'them', 'films', 'movies', 'way', 'make', 'could', 'too', 'any']\n"
     ]
    }
   ],
   "source": [
    "import re,string\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "train_data_path = \"./data/imdb/train.csv\"\n",
    "test_data_path =  \"./data/imdb/test.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "\n",
    "#构建管道\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)\n",
    "    text = tf.expand_dims(arr[1],axis = 0)\n",
    "    return (text,label)\n",
    "\n",
    "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "#构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "         '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens=MAX_WORDS-1, #有一个留给占位符\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text,label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:100])\n",
    "\n",
    "\n",
    "#单词编码\n",
    "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 200, 7)            70000     \n_________________________________________________________________\nconv_1 (Conv1D)              (None, 196, 16)           576       \n_________________________________________________________________\npool_1 (MaxPooling1D)        (None, 98, 16)            0         \n_________________________________________________________________\nconv_2 (Conv1D)              (None, 97, 128)           4224      \n_________________________________________________________________\npool_2 (MaxPooling1D)        (None, 48, 128)           0         \n_________________________________________________________________\nflatten (Flatten)            (None, 6144)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 6145      \n=================================================================\nTotal params: 80,945\nTrainable params: 80,945\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Input(shape=MAX_LEN))\n",
    "model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "model.add(layers.Conv1D(16,kernel_size=5,name = \"conv_1\",activation=\"relu\"))\n",
    "model.add(layers.MaxPool1D(name = \"pool_1\"))\n",
    "model.add(layers.Conv1D(128, kernel_size=2,name = \"conv_2\",activation = \"relu\"))\n",
    "model.add(layers.MaxPool1D(name = \"pool_2\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "      1/Unknown - 0s 1ms/step - loss: 0.6942 - binary_accuracy: 0.4000WARNING:tensorflow:From D:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "      2/Unknown - 0s 187ms/step - loss: 0.6924 - binary_accuracy: 0.4750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0350s vs `on_train_batch_end` time: 0.3196s). Check your callbacks.\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4480 - binary_accuracy: 0.7612 - val_loss: 0.3263 - val_binary_accuracy: 0.8608\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2472 - binary_accuracy: 0.9024 - val_loss: 0.3271 - val_binary_accuracy: 0.8678\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1735 - binary_accuracy: 0.9337 - val_loss: 0.3669 - val_binary_accuracy: 0.8672\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1135 - binary_accuracy: 0.9597 - val_loss: 0.4751 - val_binary_accuracy: 0.8600\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0685 - binary_accuracy: 0.9750 - val_loss: 0.6572 - val_binary_accuracy: 0.8520\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0384 - binary_accuracy: 0.9867 - val_loss: 0.7941 - val_binary_accuracy: 0.8560\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0205 - binary_accuracy: 0.9933 - val_loss: 0.9472 - val_binary_accuracy: 0.8560\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0158 - binary_accuracy: 0.9949 - val_loss: 1.1452 - val_binary_accuracy: 0.8490\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0159 - binary_accuracy: 0.9948 - val_loss: 1.1981 - val_binary_accuracy: 0.8514\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0138 - binary_accuracy: 0.9948 - val_loss: 1.2343 - val_binary_accuracy: 0.8530\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Nadam(),loss=tf.keras.losses.BinaryCrossentropy(),metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = os.path.join('data', 'autograph', stamp)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(ds_train,validation_data=ds_test,epochs=10,callbacks = [tensorboard_callback],workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 200), (None, 1)), types: (tf.int64, tf.int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod"
   ]
  }
 ]
}